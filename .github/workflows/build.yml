name: Build

on:
  workflow_dispatch:
  
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

env:
  GGML_NLOOP: 3
  GGML_N_THREADS: 1
  LLAMA_LOG_COLORS: 1
  LLAMA_LOG_PREFIX: 1
  LLAMA_LOG_TIMESTAMPS: 1

jobs:

  ubuntu-latest-cmake-cuda:
    runs-on: ubuntu-latest
    container: nvidia/cuda:12.6.3-devel-ubuntu24.04

    steps:
        - name: Clone
          id: checkout
          uses: actions/checkout@v4

        - name: Install dependencies
          env:
            DEBIAN_FRONTEND: noninteractive
          run: |
              apt update
              apt install -y cmake build-essential ninja-build libgomp1 git libcurl4-openssl-dev zip

        - name: ccache
          uses: hendrikmuhs/ccache-action@v1.2.18
          with:
            key: ubuntu-latest-cmake-cuda
            evict-old-files: 1d

        - name: Build with CMake
          run: |
            cmake -S . -B build -G Ninja \
              -DCMAKE_BUILD_TYPE=Release -DLLAMA_BUILD_SERVER=ON -DGGML_CUDA=ON \
              -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_F16=ON \
              -DGGML_CUDA_DMMV_X=64 -DGGML_CUDA_MMV_Y=2 -DK_QUANTS_PER_ITERATION=2 \
              -DGGML_RPC=ON -DCURL_LIBRARY="%CURL_PATH%/lib/libcurl.dll.a" -DCURL_INCLUDE_DIR="%CURL_PATH%/include" \
              -DCMAKE_CUDA_ARCHITECTURES=89-real \
              -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined \
              -DGGML_NATIVE=OFF -DLLAMA_FATAL_WARNINGS=ON
            cmake --build build --config Release -j 2
            
        - name: Pack artifact
          id: pack_artifact
          run: |
            cp LICENSE ./build/bin/
            zip -r llama-bin-ubuntu.zip ./build/bin/*
  
        - name: Upload artifact
          uses: actions/upload-artifact@v4
          with:
            path: llama-bin-ubuntu.zip
            name: llama-bin-ubuntu.zip
            
  windows-2022-cmake-cuda:
    runs-on: windows-2022

    strategy:
      matrix:
        cuda: ['12.6']

    steps:
      - name: Clone
        id: checkout
        uses: actions/checkout@v4

      - name: Install ccache
        uses: hendrikmuhs/ccache-action@v1.2.18
        with:
          key: windows-cuda-${{ matrix.cuda }}
          variant: ccache
          evict-old-files: 1d

      - name: Install Cuda Toolkit
        uses: ./.github/actions/windows-setup-cuda
        with:
          cuda_version: ${{ matrix.cuda }}

      - name: Install Ninja
        id: install_ninja
        run: |
          choco install ninja

      - name: libCURL
        id: get_libcurl
        uses: ./.github/actions/windows-setup-curl

      - name: Build
        id: cmake_build
        shell: cmd
        env:
          CURL_PATH: ${{ steps.get_libcurl.outputs.curl_path }}
        run: |
          call "C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\Build\vcvarsall.bat" x64
          cmake -S . -B build -G "Ninja Multi-Config" ^
            -DLLAMA_BUILD_SERVER=ON -DGGML_CUDA=ON -DGGML_NATIVE=OFF ^
            -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_F16=ON ^
            -DGGML_CUDA_DMMV_X=64 -DGGML_CUDA_MMV_Y=2 -DK_QUANTS_PER_ITERATION=2 ^
            -DGGML_RPC=ON -DCURL_LIBRARY="%CURL_PATH%/lib/libcurl.dll.a" -DCURL_INCLUDE_DIR="%CURL_PATH%/include"
          set /A NINJA_JOBS=%NUMBER_OF_PROCESSORS%-1
          cmake --build build --config Release -j %NINJA_JOBS% -t ggml
          cmake --build build --config Release

      - name: Pack artifact
        id: pack_artifact
        run: |
          7z a llama-bin-win-${{ matrix.cuda }}.zip .\build\bin\Release\*
        # ${{ matrix.instr }}   

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          path: llama-bin-win-${{ matrix.cuda }}.zip
          name: llama-bin-win-${{ matrix.cuda }}.zip
